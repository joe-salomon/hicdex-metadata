{"__version": 1, "token_id": 546495, "symbol": "OBJKT", "name": "positional_encoding", "description": "Unlike a recurrent neural network, a Transformer does not have state, and therefore is not sensitive to the passage of time. Information about time (e.g. a token's location in a sequence) is provided explicitly by adding a \"positional encoding\" vector to the network's input.\n\nIn \"Attention Is All You Need\" \u2014 the paper that introduced the Transformer \u2014 each dimension of the input vector is associated with a clock of a different speed. The cosine and sine of the angle of the clock's hand are added to the input, providing a sense of time without the need for state.", "artifact_uri": "ipfs://QmX7gqU49CY1fiazBhCPd1DJtwke3ZVopecii143m6MfEr", "display_uri": "ipfs://QmX7gqU49CY1fiazBhCPd1DJtwke3ZVopecii143m6MfEr", "thumbnail_uri": "ipfs://QmNrhZHUaEqxhyLfqoq1mtHSipkWHeT31LNHb1QEbDHgnc", "formats": [{"uri": "ipfs://QmX7gqU49CY1fiazBhCPd1DJtwke3ZVopecii143m6MfEr", "mimeType": "image/gif"}], "creators": ["tz1XJfAvEfT2yKWUkqokjXJW8f3aZ7KV7jct"], "tags": [""], "extra": {}}